{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_numpy, x_torch\n",
      "[0.1 0.2 0.3] tensor([0.1000, 0.2000, 0.3000])\n",
      "\n",
      "to and from numpy and pytorch\n",
      "tensor([0.1000, 0.2000, 0.3000], dtype=torch.float64) [0.1 0.2 0.3]\n",
      "\n",
      "x+y\n",
      "[3.1 4.2 5.3] tensor([3.1000, 4.2000, 5.3000])\n",
      "\n",
      "norm\n",
      "0.37416573867739417 tensor(0.3742)\n",
      "\n",
      "mean along the 0th dimension\n",
      "[2. 3.] tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# tensor Âíånumpy ÁöÑÂÖ≥Á≥ª\n",
    "\n",
    "# ÂàõÂª∫np array Âíå ÂàõÂª∫ tensor \n",
    "x_numpy = np.array([0.1, 0.2, 0.3])\n",
    "x_torch = torch.tensor([0.1, 0.2, 0.3])\n",
    "print('x_numpy, x_torch')\n",
    "print(x_numpy, x_torch)\n",
    "print()\n",
    "\n",
    "# Tensor Âíå numpy ÁöÑËΩ¨Êç¢\n",
    "print('to and from numpy and pytorch')\n",
    "print(torch.from_numpy(x_numpy), x_torch.numpy())\n",
    "print()\n",
    "\n",
    "# numpy Âíå TensorÈÉΩÂèØ‰ª•ÂÅö +-*%\n",
    "y_numpy = np.array([3,4,5.])\n",
    "y_torch = torch.tensor([3,4,5.])\n",
    "print(\"x+y\")\n",
    "print(x_numpy + y_numpy, x_torch + y_torch)\n",
    "print()\n",
    "\n",
    "# np ‰∏≠ÁöÑÊñπÊ≥ïÔºå pytorch‰∏≠ÂæàÂ§öÈÉΩÊúâ. ‰æãÂ¶Çnorm ,‰∫åÈò∂ËåÉÊï∞\n",
    "# ‰ΩÜÊúâÈÉ®ÂàÜÈ´òÁ∫ßÁöÑÁü©ÈòµËÆ°ÁÆóTensorÂπ∂‰∏çÊîØÊåÅÔºåÊØîÂ¶ÇËÆ°ÁÆóÁâπÂæÅÂÄºÁâπÂæÅÂêëÈáèÁ≠â„ÄÇ\n",
    "# Âõ†Ê≠§numpyËøòÊòØÊúâÂ≠òÂú®ÁöÑÂøÖË¶ÅÁöÑ„ÄÇ\n",
    "print(\"norm\")\n",
    "print(np.linalg.norm(x_numpy), torch.norm(x_torch))\n",
    "print()\n",
    "\n",
    "# to apply an operation along a dimension,\n",
    "# we use the dim keyword argument instead of axis\n",
    "print(\"mean along the 0th dimension\")\n",
    "x_numpy = np.array([[1,2],[3,4.]])\n",
    "x_torch = torch.tensor([[1,2],[3,4.]])\n",
    "print(np.mean(x_numpy, axis=0), torch.mean(x_torch, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ËΩ¨Êç¢Áª¥Â∫¶ torchÁî®Tensor.view(), numpyÁî®reshape\n",
    "- ÂéãÁº©Áª¥Â∫¶ÂíåÊñ∞Â¢ûÁª¥Â∫¶ Tensor.squeeze( ) / Tensor.unsqueeze( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 28, 28])\n",
      "torch.Size([10000, 3, 784])\n",
      "torch.Size([20000, 3, 392])\n",
      "torch.Size([10000, 3, 28, 28])\n",
      "torch.Size([1, 10000, 3, 28, 28])\n",
      "torch.Size([10000, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "N, C, W, H = 10000, 3, 28, 28\n",
    "X = torch.randn((N, C, W, H))\n",
    "\n",
    "print(X.shape)\n",
    "print(X.view(N, C, 784).shape)\n",
    "print(X.view(-1, C, 392).shape) # automatically choose the 0th dimension\n",
    "\n",
    "print(X.size())\n",
    "X = X.unsqueeze(0)\n",
    "print(X.size())\n",
    "X = X.squeeze(0)\n",
    "print(X.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable( ËøáÂéªÂºèÔºâ\n",
    "\n",
    "tensorÊòØPytorch‰∏≠ÈùûÂ∏∏È´òÊïàÊï∞ÊçÆÊ†ºÂºèÔºå‰ΩÜÁî®ÊõæÁªèÁöÑ tensorÊûÑÂª∫Á•ûÁªèÁΩëÁªúËøòËøúËøú‰∏çÂ§üÔºå‰∏∫‰∫ÜÊûÑÂª∫ËÆ°ÁÆóÂõæÔºåÊâÄ‰ª•VariableÊòØ‰∏çÂèØÊàñÁº∫ÁöÑÊï∞ÊçÆÂΩ¢Âºè„ÄÇVariableÊòØÂØπtensorÁöÑÂ∞ÅË£Ö„ÄÇ\n",
    "VariableÊúâ‰∏â‰∏™Â±ûÊÄßÔºö\n",
    "\n",
    "1. .dataÔºötensorÊú¨Ë∫´\n",
    "2. .gradÔºöÂØπÂ∫îtensorÁöÑÊ¢ØÂ∫¶\n",
    "3. .grad_fnÔºöËØ•VariableÊòØÈÄöËøá‰ªÄ‰πàÊñπÂºèËé∑ÂæóÁöÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c tensor(3., grad_fn=<AddBackward0>)\n",
      "d tensor(2., grad_fn=<AddBackward0>)\n",
      "e tensor(6., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = Variable(torch.tensor(2.0),requires_grad =True)\n",
    "\n",
    "a = torch.tensor(2.0, requires_grad=True) # we set requires_grad=True to let PyTorch know to keep the graph\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "c = a + b\n",
    "d = b + 1\n",
    "e = c * d\n",
    "print('c', c)\n",
    "print('d', d)\n",
    "print('e', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cpuÂíågpuÂàáÊç¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7492, 0.3611, 0.6825, 0.5192, 0.1796, 0.4305, 0.1121, 0.0865, 0.0929,\n",
      "        0.9328])\n",
      "tensor([0.7492, 0.3611, 0.6825, 0.5192, 0.1796, 0.4305, 0.1121, 0.0865, 0.0929,\n",
      "        0.9328], device='cuda:0')\n",
      "tensor([0.7492, 0.3611, 0.6825, 0.5192, 0.1796, 0.4305, 0.1121, 0.0865, 0.0929,\n",
      "        0.9328])\n"
     ]
    }
   ],
   "source": [
    "cpu = torch.device(\"cpu\")\n",
    "gpu = torch.device(\"cuda\") # ...ÊàëÁöÑÁîµËÑëÊ≤°Ë£Öcuda\n",
    "\n",
    "x = torch.rand(10)\n",
    "print(x)\n",
    "x = x.to(gpu)\n",
    "print(x)\n",
    "x = x.to(cpu)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen that PyTorch keeps the graph around for us, let's use it to compute some gradients for us.\n",
    "\n",
    "Consider the function ùëì(ùë•)=(ùë•‚àí2)^2 .\n",
    "\n",
    "Q: Compute ùëì'(ùë•) and then compute ùëì‚Ä≤(1) .\n",
    "\n",
    "We make a backward() call on the leaf variable (y) in the computation, computing all the gradients of y at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical f'(x): tensor([-2.], grad_fn=<MulBackward0>)\n",
      "PyTorch's f'(x): tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "def fp(x):\n",
    "    return 2*(x-2)\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "y = f(x)\n",
    "y.backward()\n",
    "\n",
    "print('Analytical f\\'(x):', fp(x))\n",
    "print('PyTorch\\'s f\\'(x):', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter,\tx,\tf(x),\tf'(x),\tf'(x) pytorch\n",
      "0,\t5.000,\t9.000,\t6.000,\t6.000\n",
      "1,\t3.500,\t2.250,\t3.000,\t3.000\n",
      "2,\t2.750,\t0.562,\t1.500,\t1.500\n",
      "3,\t2.375,\t0.141,\t0.750,\t0.750\n",
      "4,\t2.188,\t0.035,\t0.375,\t0.375\n",
      "5,\t2.094,\t0.009,\t0.188,\t0.188\n",
      "6,\t2.047,\t0.002,\t0.094,\t0.094\n",
      "7,\t2.023,\t0.001,\t0.047,\t0.047\n",
      "8,\t2.012,\t0.000,\t0.023,\t0.023\n",
      "9,\t2.006,\t0.000,\t0.012,\t0.012\n",
      "10,\t2.003,\t0.000,\t0.006,\t0.006\n",
      "11,\t2.001,\t0.000,\t0.003,\t0.003\n",
      "12,\t2.001,\t0.000,\t0.001,\t0.001\n",
      "13,\t2.000,\t0.000,\t0.001,\t0.001\n",
      "14,\t2.000,\t0.000,\t0.000,\t0.000\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "step_size = 0.25\n",
    "\n",
    "print('iter,\\tx,\\tf(x),\\tf\\'(x),\\tf\\'(x) pytorch')\n",
    "for i in range(15):\n",
    "    y = f(x)\n",
    "    y.backward() #ËÆ°ÁÆóÊ¢ØÂ∫¶\n",
    "    \n",
    "    print('{},\\t{:.3f},\\t{:.3f},\\t{:.3f},\\t{:.3f}'.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))\n",
    "    x.data = x.data - step_size * x.grad # perform a GD update step\n",
    "    \n",
    "    # We need to zero the grad variable since the backward()\n",
    "    # call accumulates the gradients in .grad instead of overwriting.\n",
    "    # The detach_() is for efficiency. You do not need to worry too much about it.\n",
    "    x.grad.detach_()\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.Module\n",
    "\n",
    "Module is PyTorch's way of performing operations on tensors. Modules are implemented as subclasses of the torch.nn.Module class. All modules are callable and can be composed together to create complex functions.\n",
    "\n",
    "### nn.Linear( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_tensor torch.Size([2, 3])\n",
      "transormed torch.Size([2, 4])\n",
      "\n",
      "We can see that the weights exist in the background\n",
      "\n",
      "W: Parameter containing:\n",
      "tensor([[-0.3501, -0.4159,  0.5708],\n",
      "        [ 0.4350,  0.4894,  0.3997],\n",
      "        [ 0.2655, -0.2392,  0.0812],\n",
      "        [-0.0085,  0.3861, -0.5370]], requires_grad=True)\n",
      "b: Parameter containing:\n",
      "tensor([ 0.5384, -0.1964, -0.2239, -0.3394], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "d_in = 3\n",
    "d_out = 4\n",
    "linear_module = nn.Linear(d_in, d_out)\n",
    "example_tensor = torch.tensor([[1., 2., 3.], [4. ,5. ,6.]])\n",
    "\n",
    "transformed = linear_module(example_tensor)\n",
    "\n",
    "print('example_tensor', example_tensor.shape)\n",
    "print('transormed', transformed.shape)\n",
    "print()\n",
    "print('We can see that the weights exist in the background\\n')\n",
    "print('W:', linear_module.weight)\n",
    "print('b:', linear_module.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "Relu, Tanh, Sigmoid   \n",
    "need to be instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_tensor tensor([-1.,  1.,  0.])\n",
      "activated tensor([0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "activation_fn = nn.ReLU() # we instantiate an instance of the ReLU module\n",
    "example_tensor = torch.tensor([-1.0, 1.0, 0.0])\n",
    "activated = activation_fn(example_tensor)\n",
    "print('example_tensor', example_tensor)\n",
    "print('activated', activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_tensor tensor([-1.,  1.,  0.])\n",
      "activated tensor([-0.7616,  0.7616,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "activation_fn = nn.Tanh() # we instantiate an instance of the Tanh module\n",
    "example_tensor = torch.tensor([-1.0, 1.0, 0.0])\n",
    "activated = activation_fn(example_tensor)\n",
    "print('example_tensor', example_tensor)\n",
    "print('activated', activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_tensor tensor([-1.,  1.,  0.])\n",
      "activated tensor([0.2689, 0.7311, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "activation_fn = nn.Sigmoid() # we instantiate an instance of the sigmoid module\n",
    "example_tensor = torch.tensor([-1.0, 1.0, 0.0])\n",
    "activated = activation_fn(example_tensor)\n",
    "print('example_tensor', example_tensor)\n",
    "print('activated', activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "\n",
    "When working with images, we often want to use convolutions to extract features using convolutions. PyTorch implments this for us in the torch.nn.Conv2d module. It expects the input to have a specific dimension  (ùëÅ,ùê∂ùëñùëõ,ùêªùëñùëõ,ùëäùëñùëõ)  where  ùëÅ  is batch size,  ùê∂ùëñùëõ  is the number of channels the image has, and  ùêªùëñùëõ,ùëäùëñùëõ  are the image height and width respectively.\n",
    "\n",
    "We can modify the convolution to have different properties with the parameters:\n",
    "\n",
    "kernel_size\n",
    "stride\n",
    "padding\n",
    "They can change the output dimension so be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an entire mnist digit\n",
    "image = np.array([0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.3803922 , 0.37647063, 0.3019608 ,0.46274513, 0.2392157 , 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.3529412 , 0.5411765 , 0.9215687 ,0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 ,0.9843138 , 0.9843138 , 0.9725491 , 0.9960785 , 0.9607844 ,0.9215687 , 0.74509805, 0.08235294, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.54901963,0.9843138 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.7411765 , 0.09019608, 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.8862746 , 0.9960785 , 0.81568635,0.7803922 , 0.7803922 , 0.7803922 , 0.7803922 , 0.54509807,0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 ,0.5019608 , 0.8705883 , 0.9960785 , 0.9960785 , 0.7411765 ,0.08235294, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.14901961, 0.32156864, 0.0509804 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.13333334,0.8352942 , 0.9960785 , 0.9960785 , 0.45098042, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.32941177, 0.9960785 ,0.9960785 , 0.9176471 , 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.32941177, 0.9960785 , 0.9960785 , 0.9176471 ,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.4156863 , 0.6156863 ,0.9960785 , 0.9960785 , 0.95294124, 0.20000002, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.09803922, 0.45882356, 0.8941177 , 0.8941177 ,0.8941177 , 0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.94117653, 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.26666668, 0.4666667 , 0.86274517,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.5568628 ,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.14509805, 0.73333335,0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 , 0.8745099 ,0.8078432 , 0.8078432 , 0.29411766, 0.26666668, 0.8431373 ,0.9960785 , 0.9960785 , 0.45882356, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.4431373 , 0.8588236 , 0.9960785 , 0.9490197 , 0.89019614,0.45098042, 0.34901962, 0.12156864, 0., 0.,0., 0., 0.7843138 , 0.9960785 , 0.9450981 ,0.16078432, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.6627451 , 0.9960785 ,0.6901961 , 0.24313727, 0., 0., 0.,0., 0., 0., 0., 0.18823531,0.9058824 , 0.9960785 , 0.9176471 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.07058824, 0.48627454, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.32941177, 0.9960785 , 0.9960785 ,0.6509804 , 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.54509807, 0.9960785 , 0.9333334 , 0.22352943, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.8235295 , 0.9803922 , 0.9960785 ,0.65882355, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.9490197 , 0.9960785 , 0.93725497, 0.22352943, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.34901962, 0.9843138 , 0.9450981 ,0.3372549 , 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.01960784,0.8078432 , 0.96470594, 0.6156863 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.01568628, 0.45882356, 0.27058825,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.], dtype=np.float32)\n",
    "image_torch = torch.from_numpy(image).view(1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFGVJREFUeJzt3XuQ3WV9x/H3Zze7CckGkkDInQCRq4wGDDdxlBYveGvQDlVqLc44xlqdaoe2UsYRZioz6FTFGR3HqCgioLR4YSxVItoirSKBpiQQMBgSsknIhSQkIWQ3u/vtH/uLXcKe59n7Ocvzec1k9uzv+9vn992TfHIuv+c8P0UEZlaepno3YGb14fCbFcrhNyuUw29WKIffrFAOv1mhHP46kbRB0htr1C6W1D7WPR3Rw7clfaZG7X2S7hnrnmxkOfw2aBFxa0S8ud592PA4/C8zkiYMZJuZw19f50p6TNJuSd+SNKm/nSSFpFf0+f4PT8kPv0SQ9ElJzwDf6m9bte87JK2StEfSf0t6VZ8xz5b0sKR9kr4P9NtLte8HJN1/RH9/LWld9fP/JGmRpF9L2ivpDkmt1b7TJf1E0o7q9/6JpPl9xjpJ0n3VOD+X9BVJ3+1Tv6DqfY+k/5V08eDvdgOHv97eB7wFWAScCnxqiOPMBmYAC4Fl/W2TdA5wE/Bh4Fjga8BdkiZWwfwRcEv1M/8C/Okge7gUeA1wAfAPwPLq91sAnAVcUe3XRO9/RguBE4AXgC/3Gec24LdVj9cB7z9ckDQP+DfgM1WffwfcKWnmIHs1HP56+3JEbIqIXcD1/H9ABqsHuDYiOiLihRrbPgR8LSIeiIjuiLgZ6KA3rBcALcCNEXEoIv4VeHCQPXw2IvZGxKPAGuCeiFgfEc8B/w6cDRARz0bEnRFxICL2Vb/3GwAknQCcC3w6Ijoj4n7grj7H+Avg7oi4OyJ6ImIFsBJ42yB7NRz+etvU5/ZGYO4Qx9kREQcz2xYCV1VPl/dI2kPvo/Lc6s/mePGnvDYOsodtfW6/0M/3bQCSJkv6mqSNkvYC9wHTJDVXfeyKiAN9frbvfbQQuPyI3+F1wJxB9mo4/PW2oM/tE4AtNfY7AEzu8/3sI+r9fTTzyG2bgOsjYlqfP5Mj4nZgKzBPko7oZzRcBZwGnB8RRwOvr7ar6mOGpL6/a9/7aBNwyxG/w5SIuGGUen1Zc/jr66OS5kuaAVwDfL/GfquAP5fULOlSqqfJg/R14K8kna9eUyS9XdJU4NdAF/A3kiZIejdw3hCOMRBT6X0msKf6va89XIiIjfQ+jb9OUqukC4F39vnZ7wLvlPSW6r6YVL25OR8bNIe/vm4D7gHWV3/6nVQDfJzeEOyh9020Hw32QBGxkt7X/V8GdgNPAh+oap3Au6vvdwPvAX4w2GMM0I3AUcBO4DfAT4+ovw+4EHiW3vvj+/S+N0FEbAKW0vsf5Q56nwn8Pf53PCTyYh7WyKrTjo9HxLXZnW1Q/D+mNRRJ51ZzBJqqlzhLGcIzHcvzzC9rNLPpfclxLNAOfCQi/qe+Lb08+Wm/WaH8tN+sUGP6tL9VE2MSU8bykGZFOcjzdEaH8nsOM/zVGzJfApqBb+QmW0xiCufrkuEc0swSHoh7B7zvkJ/2V9MxvwK8FTgTuELSmUMdz8zG1nBe858HPFl9eKMT+B69p2XMbBwYTvjn8eIPXbRX28xsHBjOa/7+3lR4yXlDScuoPmM+6UWfTTGzehrOI387L/7E1Xz6+VRaRCyPiCURsaSFicM4nJmNpOGE/0HglGrZpVbgvbx44QUza2BDftofEV2SPgb8jN5TfTdVq7iY2TgwrPP8EXE3cPcI9WJmY8jTe80K5fCbFcrhNyuUw29WKIffrFAOv1mhHH6zQjn8ZoVy+M0K5fCbFcrhNyuUw29WKIffrFAOv1mhHH6zQjn8ZoVy+M0K5fCbFcrhNyuUw29WKIffrFAOv1mhHH6zQjn8ZoVy+M0K5fCbFcrhNyuUw29WKIffrFAOv1mhHH6zQjn8ZoWaUO8GrH9Nkybl95k+Lb3DxNYR6qa2eP6F7D49u3enx+jqGql2bBCGFX5JG4B9QDfQFRFLRqIpMxt9I/HI/0cRsXMExjGzMeTX/GaFGm74A7hH0kOSlvW3g6RlklZKWnmIjmEezsxGynCf9l8UEVskHQ+skPR4RNzXd4eIWA4sBzhaM2KYxzOzETKsR/6I2FJ93Q78EDhvJJoys9E35PBLmiJp6uHbwJuBNSPVmJmNruE87Z8F/FDS4XFui4ifDqcZTZyY3ad55nHJevfMzLlvoPPY9Dn0non1fx+04+jm7D7756X77D5q+H2E0vWJ6VP4AExt707Wp7QfyI7R9NSWZL372V3pAcKvOI805PBHxHrg1SPYi5mNofo/xJlZXTj8ZoVy+M0K5fCbFcrhNyuUw29WKIffrFBjupiHmppomjylZr371a/IjrH1/No/D7D39EPZMWYtSM9MOW7y89kxRtsJE/M9nDple7J+THN+oY2cJvUk65s7pmfHeOS5ecn6o7+bnx1jzr2nJOvT//OpZL1rW/q+AoqbCORHfrNCOfxmhXL4zQrl8JsVyuE3K5TDb1Yoh9+sUGN70Y7WFrSw9jnfjW+fnB3itZesTtbfOP2x7BgntuxI1qc2dWbHyDkY6cU41ncen6xv7EwvWgJwsKclWd/VlZ4TMRBTmw8m6+e1rc+O8d7pv03WH587KzvGp6f9SbIeTScl6zN+mT1Efi7Ay2wegB/5zQrl8JsVyuE3K5TDb1Yoh9+sUA6/WaEcfrNCjel5/mgSPZNba9YPHZ3+7DhAR3e65Vs2X5AdY/v+tmS9u2f4/yd2dKb77NyWntNw1Jb8RTsm5K91MWy5C38cWJC+IAfAOa/6fbL+kbn5k/DXL/5xsv7Jrncn6637F2aPMfk/0ndoz7592THGEz/ymxXK4TcrlMNvViiH36xQDr9ZoRx+s0I5/GaFcvjNCjW2F+041E3z1l016/N+kV984ok1pyfrk3blJwpN39WVrKtn+Is26FC6jwl7nkvWm/bkJ5REx/AXHclRa3rBkO7Z+Yt2PPnaU5P1z102KTvGZ0++M1n/28X3Jus3Pv2O7DFOeXJ2eoe1hU3ykXSTpO2S1vTZNkPSCknrqq/5fwFm1lAG8rT/28ClR2y7Grg3Ik4B7q2+N7NxJBv+iLgPOPK5+lLg5ur2zcBlI9yXmY2yob7hNysitgJUX2uuRilpmaSVklZ29gz/wpFmNjJG/d3+iFgeEUsiYklrU+YjYmY2ZoYa/m2S5gBUXwdw/WMzayRDDf9dwJXV7SuB9IetzazhZM/zS7oduBg4TlI7cC1wA3CHpA8CTwOXD+RgcegQXVu31ay3/Tx/HnVq5rxzPJ9f4aLnYPpCFGMhNxshP1uhQST+Pg+b03FKsr5u4fzsGI/PT5+DX9q2Nlm/6ZUXZo/RMefoZH1C+hDjTjb8EXFFjdIlI9yLmY0hT+81K5TDb1Yoh9+sUA6/WaEcfrNCOfxmhXL4zQo1pot5ANBT+wovL7cropSgKTPpCiAmpf+ZqVvZMQ5G+jgzmycm6ydNq72IzGHPTE8vS9EyIf17RFd6kZhG40d+s0I5/GaFcvjNCuXwmxXK4TcrlMNvViiH36xQY3+e3wZG+XPfzVOnpneYNys7Rufs9Bhdk5qT9Y5p6TrA7jPSv8u8s7dmxzi9Nb3PBNJ9nHPMpuwxbj01fXGRY+akFxTp2tSePUYj8SO/WaEcfrNCOfxmhXL4zQrl8JsVyuE3K5TDb1Yon+cfJWppTdabZkxL1mPOcdlj7D4jfZGJnWfn5wq0nbY7WT++bX+yPnfS89ljvGfaU8n6ayevy45xZkvtdSAAmjUpWf/jtseyx/jGGRcl613zZqQH8Hl+MxsPHH6zQjn8ZoVy+M0K5fCbFcrhNyuUw29WKIffrFCe5NOfpvTCEM0zj80O0Xnm/GR951npSSl7T01PagE46YwtyfrV836bHeM1kzYm6xOV7mNHz+TsMdZ1pBfBeLxjTnaMaU3pPhepJztGTmQuHqLuSP/8sDsYW9lHfkk3SdouaU2fbddJ2ixpVfXnbaPbppmNtIE87f82cGk/278YEYurP3ePbFtmNtqy4Y+I+4D8hc7MbFwZzht+H5P0SPWyoOYVDiUtk7RS0spDdAzjcGY2koYa/q8Ci4DFwFbg87V2jIjlEbEkIpa0kL6SqpmNnSGFPyK2RUR3RPQAXwfOG9m2zGy0DSn8kvqem3kXsKbWvmbWmLLn+SXdDlwMHCepHbgWuFjSYnpPbW4APjyKPY64psnpc9M6aUGyvv3CzKIOwO43HEzW33L6w8n6qZOfyR6jJXMOvr0z3+eKXWcm65v2pRcd2fbsMdljNG9Iz2k41JY/Q/6u16XnLPzj8b9K1h/tWJQ9RuvmzAIsu9Pve+dnZjSWbPgj4op+Nn9zFHoxszHk6b1mhXL4zQrl8JsVyuE3K5TDb1Yoh9+sUA6/WaGKXMyjadbMZL39TenFOtouzU/A+cs5jyfre7vSE1++t3FJ9hjb16f7nNyeXpQEYMrm9ASbo3Z2Jesn7+nMHmPCrh3J+u5z8lcneuSsecn6gZnp32P1gfTiKgBtmzI7PLsnO8Z44kd+s0I5/GaFcvjNCuXwmxXK4TcrlMNvViiH36xQRZ7nj9aWZL0zvX4FLU35C0R899H0ymZHPZReUGTG44eyxzht03PJetOufdkxep7bm67v358eIAZwqYpp6QU/uial510AzJmc/l0PZdpYvXtu9hhtm9PLcXRn7qvxxo/8ZoVy+M0K5fCbFcrhNyuUw29WKIffrFAOv1mhijzPz/Znk+VZD9S87igAe5+Zk6wDzNuY/hz8lFXrk/XuHTuzx+jpSh8jPxthbOiYo5P15+cqO8biqe3J+u8Ppf/O1j81K3uM0545kKxHz3i7LEeaH/nNCuXwmxXK4TcrlMNvViiH36xQDr9ZoRx+s0I5/GaFKnKST/ee9MUXJv/XE+n6w+kLbgDEvvQiGF0H0hNKxovmY2dk99n7mvRCGl1nPZ8dY2FretLTnbvSFzk55pH0Ai4Aze1PJ+vpKVXjT/aRX9ICSb+UtFbSo5I+Xm2fIWmFpHXV1/QUKzNrKAN52t8FXBURZwAXAB+VdCZwNXBvRJwC3Ft9b2bjRDb8EbE1Ih6ubu8D1gLzgKXAzdVuNwOXjVaTZjbyBvWGn6QTgbOBB4BZEbEVev+DAI6v8TPLJK2UtPIQHcPr1sxGzIDDL6kNuBP4REQMeBnTiFgeEUsiYkkLE4fSo5mNggGFX1ILvcG/NSJ+UG3eJmlOVZ8DbB+dFs1sNAzk3X4B3wTWRsQX+pTuAq6sbl8J/Hjk2zOz0TKQ8/wXAe8HVktaVW27BrgBuEPSB4GngctHp8VRkLnQRPfezKuaXL0g3YvmZfdpvzS9rMinFv8sfxzSC36seOzMZH3Rqhfyx9iZXuTl5SYb/oi4H2re85eMbDtmNlY8vdesUA6/WaEcfrNCOfxmhXL4zQrl8JsVqsjP89vAaWJ6Svbek6Zkxzj/lb9L1s89akN2jE9vXJqsT/tNa7Leui59kRSArsxFUF5u/MhvViiH36xQDr9ZoRx+s0I5/GaFcvjNCuXwmxXK4TcrlCf5WFLz3NnJ+u7T848fb2rblqzfsuvC7BhP/GJRsn7i/buS9dIW6hgIP/KbFcrhNyuUw29WKIffrFAOv1mhHH6zQjn8ZoXyeX5L6j7u6GS9Y0b6ghwA921/RbK+6ZE52TFO/vmBZD3WPZWuF7ZQx0D4kd+sUA6/WaEcfrNCOfxmhXL4zQrl8JsVyuE3K5TDb1ao7CQfSQuA7wCzgR5geUR8SdJ1wIeAHdWu10TE3aPVqNVH094XkvVjV7Vlx9j72Nxk/eTV6Qk8AM2r01fc6enoyI5hLzaQGX5dwFUR8bCkqcBDklZUtS9GxD+PXntmNlqy4Y+IrcDW6vY+SWuBeaPdmJmNrkG95pd0InA28EC16WOSHpF0k6TpI9ybmY2iAYdfUhtwJ/CJiNgLfBVYBCym95nB52v83DJJKyWtPIRfl5k1igGFX1ILvcG/NSJ+ABAR2yKiOyJ6gK8D5/X3sxGxPCKWRMSSFtKXezazsZMNvyQB3wTWRsQX+mzv+znMdwFrRr49MxstA3m3/yLg/cBqSauqbdcAV0haDASwAfjwqHRoZqNCETF2B5N2ABv7bDoO2DlmDQyd+xxZ46HP8dAjvLTPhRExcyA/OKbhf8nBpZURsaRuDQyQ+xxZ46HP8dAjDK9PT+81K5TDb1aoeod/eZ2PP1Duc2SNhz7HQ48wjD7r+prfzOqn3o/8ZlYnDr9ZoeoWfkmXSnpC0pOSrq5XHzmSNkhaLWmVpJX17uew6sNU2yWt6bNthqQVktZVX+v6YasaPV4naXN1f66S9LZ69lj1tEDSLyWtlfSopI9X2xvt/qzV55Du07q85pfUDPwOeBPQDjwIXBERj415MxmSNgBLIqKhJnxIej2wH/hORJxVbfscsCsibqj+Q50eEZ9ssB6vA/Y30joQ1VT1OX3XrAAuAz5AY92ftfr8M4Zwn9brkf884MmIWB8RncD3gKV16mVcioj7gF1HbF4K3Fzdvpnefxh1U6PHhhMRWyPi4er2PuDwmhWNdn/W6nNI6hX+ecCmPt+307gLhARwj6SHJC2rdzMZs6rFVw4vwnJ8nfuppWHXgThizYqGvT9HYm2NeoVf/Wxr1HOOF0XEOcBbgY9WT2Vt6Aa0DkQ99LNmRUMa6toaR6pX+NuBBX2+nw9sqVMvSRGxpfq6HfghNdYtaBDbDn/Uuvq6vc79vMRA14EYa/2tWUED3p/DWVvjSPUK/4PAKZJOktQKvBe4q0691CRpSvXGCpKmAG+msdctuAu4srp9JfDjOvbSr0ZcB6LWmhU02P050mtr1G2GX3U64kagGbgpIq6vSyMJkk6m99Eeetc+uK1R+pR0O3AxvR/p3AZcC/wIuAM4AXgauDwi6vaGW40eL6b36ekf1oE4/Lq6XiS9DvgVsJre5emhd82KB2is+7NWn1cwhPvU03vNCuUZfmaFcvjNCuXwmxXK4TcrlMNvViiH36xQDr9Zof4P3aHr+7uhresAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a gaussian blur kernel\n",
    "gaussian_kernel = torch.tensor([[1., 2, 1],[2, 4, 2],[1, 2, 1]]) / 16.0\n",
    "\n",
    "conv = nn.Conv2d(1, 1, 3)\n",
    "# manually set the conv weight\n",
    "conv.weight.data[:] = gaussian_kernel\n",
    "\n",
    "convolved = conv(image_torch)\n",
    "\n",
    "plt.title('original image')\n",
    "plt.imshow(image_torch.view(28,28).detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "#ÂΩìÊàë‰ª¨ËÆ≠ÁªÉÁΩëÁªúÁöÑÊó∂ÂÄôÂèØËÉΩÂ∏åÊúõ‰øùÊåÅ‰∏ÄÈÉ®ÂàÜÁöÑÁΩëÁªúÂèÇÊï∞‰∏çÂèòÔºå\n",
    "# Âè™ÂØπÂÖ∂‰∏≠‰∏ÄÈÉ®ÂàÜÁöÑÂèÇÊï∞ËøõË°åË∞ÉÊï¥Ôºõ\n",
    "#ÊàñËÄÖÂÄºËÆ≠ÁªÉÈÉ®ÂàÜÂàÜÊîØÁΩëÁªúÔºåÂπ∂‰∏çËÆ©ÂÖ∂Ê¢ØÂ∫¶ÂØπ‰∏ªÁΩëÁªúÁöÑÊ¢ØÂ∫¶ÈÄ†ÊàêÂΩ±Âìç\n",
    "#ËøôÊó∂ÂÄôÊàë‰ª¨Â∞±ÈúÄË¶Å‰ΩøÁî®detach()ÂáΩÊï∞Êù•ÂàáÊñ≠‰∏Ä‰∫õÂàÜÊîØÁöÑÂèçÂêë‰º†Êí≠\n",
    "\n",
    "plt.title('blurred image')\n",
    "plt.imshow(convolved.view(26,26).detach().numpy()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "What if we want to compose modules together ?\n",
    "torch.nn.Sequential offer a excellent interface of composing modules elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "d_in = 3\n",
    "d_hidden = 4\n",
    "d_out = 1\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(d_in ,d_hidden),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(d_hidden, d_out),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "example_tensor = torch.tensor([[1.,2,3],[4,5,6]])\n",
    "transformed = model(example_tensor)\n",
    "print('transformed', transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2804, -0.3282,  0.3279],\n",
      "        [ 0.2409, -0.3137,  0.3888],\n",
      "        [ 0.4919, -0.2265, -0.4070],\n",
      "        [ 0.0780, -0.5210,  0.4157]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4127, 0.1463, 0.0731, 0.4998], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3509,  0.1902,  0.3770, -0.4909]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3985], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "params = model.parameters()\n",
    "\n",
    "for param in params:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Like MSELOSS, CrossEntropyLoss, Negative log likelihood and ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3333)\n"
     ]
    }
   ],
   "source": [
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "prediction = torch.tensor([[-1., 0, -1]])\n",
    "target = torch.tensor([[1., 0, -1]])\n",
    "\n",
    "loss = mse_loss_fn(prediction, target)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross entropy\n",
    "\n",
    "A classification task and will use the cross entropy loss. PyTorch implements a version of the cross entropy loss in one module called CrossEntropyLoss.\n",
    "\n",
    "input: The first parameter to CrossEntropyLoss is the output of our network. It expects a real valued tensor of dimensions  (ùëÅ,ùê∂)  where  ùëÅ  is the minibatch size and  ùê∂  is the number of classes. In our case  ùëÅ=3  and  ùê∂=2 . The values along the second dimension correspond to raw unnormalized scores for each class. The CrossEntropyLoss module does the softmax calculation for us, so we do not need to apply our own softmax to the output of our neural network.\n",
    "\n",
    "output: The second parameter to CrossEntropyLoss is the true label. It expects an integer valued tensor of dimension  (ùëÅ) . The integer at each element corresponds to the correct class. In our case, the \"correct\" class labels are class 0, class 1, and class 1.\n",
    "Try out the loss function on three toy predictions. The true class labels are  ùë¶=[1,1,0] . The first two examples correspond to predictions that are \"correct\" in that they have higher raw scores for the correct class. The second example is \"more confident\" in the prediction, leading to a smaller loss. The last two examples are incorrect predictions with lower and higher confidence respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1269)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "input = torch.tensor([[-1., 1],[-1, 1],[1, -1]]) # raw scores correspond to the correct class\n",
    "# input = torch.tensor([[-3., 3],[-3, 3],[3, -3]]) # raw scores correspond to the correct class with higher confidence\n",
    "# input = torch.tensor([[1., -1],[1, -1],[-1, 1]]) # raw scores correspond to the incorrect class\n",
    "# input = torch.tensor([[3., -3],[3, -3],[-3, 3]]) # raw scores correspond to the incorrect class with incorrectly placed confidence\n",
    "\n",
    "target = torch.tensor([1, 1, 0])\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.optim\n",
    "\n",
    "PyTorch implements a number of gradient-based optimization methods in torch.optim, including Gradient Descent. At the minimum, it takes in the model parameters and a learning rate.\n",
    "\n",
    "For Momentum part. https://distill.pub/2017/momentum/ would be a good start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model params before: Parameter containing:\n",
      "tensor([[-0.3930]], requires_grad=True)\n",
      "model params after: Parameter containing:\n",
      "tensor([[-0.3301]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create a simple model\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# create a simple dataset\n",
    "X_simple = torch.tensor([[1.]])\n",
    "y_simple = torch.tensor([[2.]])\n",
    "\n",
    "# create our optimizer\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "# or taking momentum into consideration\n",
    "momentum = 0.5\n",
    "optim = torch.optim.SGD(model.parameters(), \n",
    "                        lr=1e-2, momentum=momentum)\n",
    "\n",
    "# or other fancy optimizer like Adam\n",
    "optim_adam = torch.optim.Adam(model.parameters(),lr=1e-2)\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "y_hat = model(X_simple)\n",
    "print('model params before:', model.weight)\n",
    "loss = mse_loss_fn(y_hat, y_simple)\n",
    "optim.zero_grad()\n",
    "loss.backward()\n",
    "optim.step()\n",
    "\n",
    "# a -learning_rate * grad.\n",
    "print('model params after:', model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n",
    "\n",
    "torch.utils.data.Dataset is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n",
    "\n",
    "__len__ so that len(dataset) returns the size of the dataset.\n",
    "__getitem__ to support the indexing such that dataset[i] can be used to get  ùëñ \\ th sample\n",
    "Let's create a dataset class for our face landmarks dataset. We will read the csv in __init__ but leave the reading of images to __getitem__. This is memory efficient because all the images are not stored in the memory at once but read as required.\n",
    "\n",
    "Sample of our dataset will be a dict {'image': image, 'landmarks': landmarks}. Our dataset will take an optional argument transform so that any required processing can be applied on the sample. We will see the usefulness of transform in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class FakeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y #golden_standard\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [tensor([[0.7307, 0.9145, 0.5067, 0.6182, 0.1013, 0.3079, 0.5304, 0.6617, 0.6820,\n",
      "         0.0989],\n",
      "        [0.4947, 0.2238, 0.1146, 0.5315, 0.7406, 0.4817, 0.0520, 0.2119, 0.0584,\n",
      "         0.4059],\n",
      "        [0.0297, 0.8084, 0.9956, 0.8194, 0.4930, 0.0105, 0.2270, 0.4995, 0.4853,\n",
      "         0.2102],\n",
      "        [0.6187, 0.9844, 0.1446, 0.6054, 0.4341, 0.2525, 0.8731, 0.9367, 0.4784,\n",
      "         0.5213]], dtype=torch.float64), tensor([0.6800, 0.4747, 0.7230, 0.6647])]\n",
      "1 [tensor([[0.9110, 0.9495, 0.5603, 0.3630, 0.7557, 0.9070, 0.4803, 0.4039, 0.4122,\n",
      "         0.9527],\n",
      "        [0.2849, 0.5019, 0.7625, 0.8943, 0.9171, 0.8171, 0.7113, 0.3369, 0.8075,\n",
      "         0.9609],\n",
      "        [0.4339, 0.3313, 0.8928, 0.7215, 0.7342, 0.2347, 0.8720, 0.1846, 0.4683,\n",
      "         0.9454],\n",
      "        [0.9125, 0.4427, 0.5902, 0.9490, 0.1813, 0.6399, 0.5959, 0.0893, 0.3081,\n",
      "         0.3834]], dtype=torch.float64), tensor([0.6523, 0.9180, 0.0395, 0.0575])]\n",
      "2 [tensor([[0.7529, 0.4594, 0.0279, 0.8599, 0.9538, 0.6852, 0.0317, 0.9228, 0.3858,\n",
      "         0.9148],\n",
      "        [0.0582, 0.9477, 0.7513, 0.7206, 0.5122, 0.3967, 0.4122, 0.4335, 0.5351,\n",
      "         0.9113],\n",
      "        [0.8124, 0.9145, 0.8780, 0.3271, 0.3118, 0.8112, 0.1098, 0.4667, 0.0860,\n",
      "         0.1498],\n",
      "        [0.5757, 0.1844, 0.4541, 0.4112, 0.5591, 0.4528, 0.9571, 0.8088, 0.0303,\n",
      "         0.9455]], dtype=torch.float64), tensor([0.8449, 0.9932, 0.9756, 0.3823])]\n",
      "3 [tensor([[0.2273, 0.5496, 0.7936, 0.2198, 0.7919, 0.0495, 0.0406, 0.7310, 0.1621,\n",
      "         0.0267],\n",
      "        [0.8747, 0.2290, 0.6561, 0.0986, 0.5429, 0.9025, 0.0947, 0.9344, 0.8254,\n",
      "         0.7855],\n",
      "        [0.2132, 0.6670, 0.3228, 0.0789, 0.3930, 0.7945, 0.3614, 0.7774, 0.1442,\n",
      "         0.6295],\n",
      "        [0.4451, 0.4931, 0.1777, 0.5891, 0.6899, 0.7545, 0.9547, 0.1223, 0.7810,\n",
      "         0.8989]], dtype=torch.float64), tensor([0.9141, 0.7251, 0.5867, 0.1818])]\n",
      "4 [tensor([[0.4705, 0.6977, 0.5118, 0.5094, 0.0279, 0.1453, 0.8515, 0.2348, 0.7536,\n",
      "         0.9441],\n",
      "        [0.7786, 0.7114, 0.1803, 0.6543, 0.4851, 0.5443, 0.4212, 0.9344, 0.3777,\n",
      "         0.4269],\n",
      "        [0.9601, 0.2711, 0.0187, 0.7211, 0.7611, 0.6132, 0.7957, 0.3658, 0.9424,\n",
      "         0.7756],\n",
      "        [0.8856, 0.7203, 0.6138, 0.4376, 0.0868, 0.7462, 0.7532, 0.5153, 0.0755,\n",
      "         0.1954]], dtype=torch.float64), tensor([0.2523, 0.2535, 0.9998, 0.1059])]\n",
      "5 [tensor([[0.3367, 0.5260, 0.9450, 0.8661, 0.2312, 0.0576, 0.9111, 0.9937, 0.3611,\n",
      "         0.3880],\n",
      "        [0.7283, 0.3695, 0.5846, 0.5280, 0.8117, 0.3133, 0.6775, 0.4613, 0.8165,\n",
      "         0.8897],\n",
      "        [0.3488, 0.7455, 0.4576, 0.8145, 0.8697, 0.4937, 0.0334, 0.2588, 0.4789,\n",
      "         0.0810],\n",
      "        [0.7622, 0.2379, 0.0322, 0.7635, 0.7186, 0.4269, 0.0188, 0.0448, 0.9658,\n",
      "         0.4856]], dtype=torch.float64), tensor([0.9426, 0.0480, 0.5246, 0.9229])]\n",
      "6 [tensor([[0.4259, 0.7722, 0.1232, 0.3724, 0.4366, 0.7133, 0.2918, 0.4254, 0.5142,\n",
      "         0.5632],\n",
      "        [0.8166, 0.5774, 0.3994, 0.6776, 0.8247, 0.8521, 0.9975, 0.1013, 0.0275,\n",
      "         0.1229],\n",
      "        [0.7414, 0.0689, 0.5003, 0.3418, 0.4715, 0.1115, 0.0747, 0.2238, 0.9743,\n",
      "         0.7703],\n",
      "        [0.8483, 0.3902, 0.7024, 0.3780, 0.1349, 0.1686, 0.9918, 0.5103, 0.9905,\n",
      "         0.6929]], dtype=torch.float64), tensor([0.7380, 0.7002, 0.4496, 0.4982])]\n",
      "7 [tensor([[0.4297, 0.8196, 0.4156, 0.0597, 0.4824, 0.1266, 0.9368, 0.9504, 0.4159,\n",
      "         0.4148],\n",
      "        [0.3050, 0.4881, 0.7781, 0.6174, 0.8760, 0.9185, 0.9685, 0.2151, 0.3790,\n",
      "         0.8241],\n",
      "        [0.5794, 0.2251, 0.4198, 0.2312, 0.5810, 0.8970, 0.8765, 0.4935, 0.2890,\n",
      "         0.3744],\n",
      "        [0.0060, 0.0688, 0.9651, 0.9888, 0.2635, 0.0777, 0.8307, 0.4341, 0.5665,\n",
      "         0.1160]], dtype=torch.float64), tensor([0.8485, 0.7624, 0.3078, 0.3912])]\n",
      "8 [tensor([[0.1183, 0.5992, 0.6557, 0.7984, 0.3296, 0.8172, 0.5895, 0.8113, 0.9764,\n",
      "         0.9612],\n",
      "        [0.8341, 0.0781, 0.9888, 0.5729, 0.6151, 0.7042, 0.1435, 0.9341, 0.8102,\n",
      "         0.0026],\n",
      "        [0.1572, 0.2661, 0.0197, 0.9125, 0.6982, 0.5271, 0.7173, 0.1980, 0.0443,\n",
      "         0.2485],\n",
      "        [0.7276, 0.9288, 0.7803, 0.9154, 0.0289, 0.1121, 0.6076, 0.8155, 0.0443,\n",
      "         0.9458]], dtype=torch.float64), tensor([0.5405, 0.7614, 0.2379, 0.7723])]\n",
      "9 [tensor([[0.5576, 0.4118, 0.6777, 0.3851, 0.0603, 0.7250, 0.5873, 0.5359, 0.1056,\n",
      "         0.0192],\n",
      "        [0.2541, 0.6497, 0.7819, 0.6554, 0.1204, 0.2811, 0.8289, 0.0866, 0.2220,\n",
      "         0.7459],\n",
      "        [0.9079, 0.4071, 0.3435, 0.6151, 0.8730, 0.9730, 0.0672, 0.0494, 0.3668,\n",
      "         0.3653],\n",
      "        [0.6786, 0.8709, 0.8085, 0.1615, 0.4993, 0.6953, 0.6841, 0.7457, 0.4263,\n",
      "         0.7370]], dtype=torch.float64), tensor([0.7536, 0.2837, 0.7462, 0.4520])]\n",
      "10 [tensor([[0.1232, 0.8055, 0.0235, 0.4874, 0.2773, 0.9101, 0.3141, 0.4099, 0.2683,\n",
      "         0.8482],\n",
      "        [0.2705, 0.8870, 0.2444, 0.7004, 0.2753, 0.1766, 0.3669, 0.8838, 0.8518,\n",
      "         0.5962],\n",
      "        [0.2674, 0.7556, 0.4407, 0.2477, 0.9839, 0.9793, 0.3795, 0.3082, 0.9375,\n",
      "         0.2775],\n",
      "        [0.8202, 0.8189, 0.5179, 0.0424, 0.2242, 0.6481, 0.5996, 0.5309, 0.1571,\n",
      "         0.5482]], dtype=torch.float64), tensor([0.7809, 0.3901, 0.1289, 0.5728])]\n",
      "11 [tensor([[0.8007, 0.7953, 0.1389, 0.5347, 0.7785, 0.1325, 0.8570, 0.6135, 0.2634,\n",
      "         0.2160],\n",
      "        [0.7269, 0.8246, 0.0365, 0.2517, 0.0217, 0.1259, 0.3705, 0.4981, 0.4738,\n",
      "         0.7833],\n",
      "        [0.6925, 0.7272, 0.0762, 0.3514, 0.3401, 0.4935, 0.3628, 0.0198, 0.6356,\n",
      "         0.4451],\n",
      "        [0.4219, 0.7656, 0.9273, 0.1997, 0.2859, 0.6416, 0.4158, 0.1287, 0.4355,\n",
      "         0.3690]], dtype=torch.float64), tensor([0.3546, 0.8480, 0.4805, 0.5140])]\n",
      "12 [tensor([[0.6124, 0.3400, 0.5839, 0.4761, 0.6583, 0.5780, 0.8815, 0.5140, 0.9592,\n",
      "         0.7215],\n",
      "        [0.0676, 0.6701, 0.5985, 0.2505, 0.9586, 0.2899, 0.7416, 0.7332, 0.3257,\n",
      "         0.8893],\n",
      "        [0.0522, 0.9845, 0.3908, 0.7639, 0.5165, 0.2400, 0.7839, 0.2315, 0.7186,\n",
      "         0.0589],\n",
      "        [0.2244, 0.9940, 0.4375, 0.5244, 0.3698, 0.4622, 0.3976, 0.3363, 0.8514,\n",
      "         0.5949]], dtype=torch.float64), tensor([0.2770, 0.5171, 0.9404, 0.5752])]\n",
      "13 [tensor([[0.2292, 0.1778, 0.5099, 0.5376, 0.1092, 0.0179, 0.2101, 0.0887, 0.5968,\n",
      "         0.8625],\n",
      "        [0.3994, 0.7813, 0.2575, 0.5126, 0.3492, 0.9857, 0.1632, 0.0827, 0.6853,\n",
      "         0.5507],\n",
      "        [0.6906, 0.0946, 0.1053, 0.5371, 0.2316, 0.0538, 0.4214, 0.3341, 0.7776,\n",
      "         0.0428],\n",
      "        [0.4535, 0.3655, 0.4885, 0.1366, 0.3185, 0.8184, 0.8516, 0.4907, 0.2691,\n",
      "         0.2393]], dtype=torch.float64), tensor([0.1990, 0.2098, 0.1860, 0.4317])]\n",
      "14 [tensor([[9.6134e-01, 5.4569e-01, 5.3136e-01, 5.2410e-01, 4.1917e-01, 9.5525e-01,\n",
      "         6.9730e-02, 4.0721e-01, 2.0313e-02, 4.6487e-01],\n",
      "        [5.3622e-01, 9.8027e-01, 6.5768e-01, 6.8099e-01, 2.9333e-01, 6.6577e-01,\n",
      "         8.7354e-01, 5.9566e-01, 2.9808e-01, 9.3027e-02],\n",
      "        [8.7942e-01, 9.9385e-01, 1.8950e-01, 1.6226e-01, 2.3371e-01, 6.7219e-01,\n",
      "         3.4254e-01, 4.0402e-01, 3.8838e-01, 8.9795e-02],\n",
      "        [1.7145e-01, 5.4899e-04, 2.7333e-01, 5.0694e-01, 2.6793e-01, 2.8822e-01,\n",
      "         6.2043e-01, 6.4175e-01, 3.0017e-01, 8.4094e-01]], dtype=torch.float64), tensor([0.5968, 0.6794, 0.7388, 0.4511])]\n",
      "15 [tensor([[0.8513, 0.3059, 0.3792, 0.3076, 0.3805, 0.8979, 0.3654, 0.2608, 0.5180,\n",
      "         0.4883],\n",
      "        [0.5432, 0.1024, 0.7364, 0.8209, 0.7703, 0.9316, 0.5186, 0.7067, 0.5635,\n",
      "         0.5913],\n",
      "        [0.5090, 0.0678, 0.9094, 0.5694, 0.1816, 0.1504, 0.5410, 0.0756, 0.0290,\n",
      "         0.7750],\n",
      "        [0.6390, 0.2169, 0.5724, 0.7563, 0.1024, 0.4056, 0.6552, 0.6060, 0.5488,\n",
      "         0.5951]], dtype=torch.float64), tensor([0.0996, 0.3155, 0.5351, 0.7094])]\n",
      "16 [tensor([[0.3048, 0.3270, 0.0395, 0.2128, 0.6435, 0.3722, 0.4633, 0.5614, 0.4564,\n",
      "         0.8187],\n",
      "        [0.2750, 0.6892, 0.7501, 0.1504, 0.7989, 0.2614, 0.2431, 0.0636, 0.6011,\n",
      "         0.7652],\n",
      "        [0.7263, 0.1930, 0.6326, 0.9790, 0.2442, 0.8348, 0.2204, 0.1109, 0.3547,\n",
      "         0.4719],\n",
      "        [0.7088, 0.2117, 0.2877, 0.8798, 0.5060, 0.9349, 0.4280, 0.8221, 0.0193,\n",
      "         0.3453]], dtype=torch.float64), tensor([0.0846, 0.4743, 0.6170, 0.8437])]\n",
      "17 [tensor([[4.1712e-01, 4.2988e-01, 9.2946e-01, 3.5262e-01, 7.7458e-01, 8.6260e-01,\n",
      "         2.7239e-01, 2.1332e-01, 3.1475e-01, 9.2387e-01],\n",
      "        [9.0125e-01, 2.6591e-01, 3.3563e-01, 8.6447e-01, 9.7485e-02, 8.2850e-02,\n",
      "         4.8661e-02, 4.1731e-01, 5.4813e-01, 3.0415e-01],\n",
      "        [8.6703e-01, 8.9114e-01, 3.9166e-01, 9.6049e-01, 4.8338e-01, 4.1305e-01,\n",
      "         1.1239e-01, 1.7774e-01, 6.8966e-01, 6.7938e-01],\n",
      "        [5.5973e-01, 4.7579e-01, 7.7370e-01, 7.7687e-01, 4.6606e-01, 1.7908e-01,\n",
      "         7.1221e-01, 2.0614e-05, 6.6843e-01, 4.6254e-01]], dtype=torch.float64), tensor([0.5196, 0.0070, 0.6558, 0.3763])]\n",
      "18 [tensor([[0.8580, 0.5997, 0.9996, 0.3457, 0.4137, 0.1671, 0.5743, 0.3647, 0.7112,\n",
      "         0.0835],\n",
      "        [0.8034, 0.4378, 0.9650, 0.3083, 0.2460, 0.1779, 0.2112, 0.5938, 0.1682,\n",
      "         0.4541],\n",
      "        [0.8742, 0.6017, 0.7195, 0.4220, 0.2250, 0.8190, 0.4339, 0.6777, 0.4213,\n",
      "         0.7722],\n",
      "        [0.1578, 0.7734, 0.3857, 0.8743, 0.1374, 0.1235, 0.1373, 0.3731, 0.1421,\n",
      "         0.6752]], dtype=torch.float64), tensor([0.0985, 0.0758, 0.2157, 0.6228])]\n",
      "19 [tensor([[0.9237, 0.5640, 0.0162, 0.9416, 0.7067, 0.8570, 0.6898, 0.4722, 0.2594,\n",
      "         0.0146],\n",
      "        [0.0126, 0.1592, 0.8489, 0.3025, 0.9220, 0.1712, 0.7374, 0.3602, 0.3654,\n",
      "         0.1391],\n",
      "        [0.8971, 0.9913, 0.4870, 0.2611, 0.5604, 0.2293, 0.1766, 0.9462, 0.6963,\n",
      "         0.6228],\n",
      "        [0.5682, 0.7996, 0.9496, 0.0213, 0.6945, 0.7873, 0.8829, 0.6120, 0.5460,\n",
      "         0.4440]], dtype=torch.float64), tensor([0.4401, 0.9734, 0.8576, 0.3426])]\n",
      "20 [tensor([[0.1732, 0.0168, 0.5730, 0.4370, 0.7326, 0.8682, 0.5582, 0.7644, 0.4996,\n",
      "         0.4806],\n",
      "        [0.6285, 0.4216, 0.0373, 0.3237, 0.0804, 0.0190, 0.5002, 0.8869, 0.4667,\n",
      "         0.7119],\n",
      "        [0.2441, 0.5309, 0.5711, 0.0775, 0.1275, 0.4759, 0.6755, 0.1798, 0.7761,\n",
      "         0.1308],\n",
      "        [0.0962, 0.7125, 0.9320, 0.1757, 0.7089, 0.6517, 0.8742, 0.7952, 0.7489,\n",
      "         0.2504]], dtype=torch.float64), tensor([0.9645, 0.2859, 0.6706, 0.1103])]\n",
      "21 [tensor([[0.4628, 0.0997, 0.4325, 0.7257, 0.2006, 0.6227, 0.3774, 0.1109, 0.1837,\n",
      "         0.4346],\n",
      "        [0.1367, 0.1416, 0.8947, 0.6155, 0.8464, 0.8894, 0.3191, 0.8565, 0.6702,\n",
      "         0.2895],\n",
      "        [0.7947, 0.8838, 0.4978, 0.8927, 0.9486, 0.0988, 0.9717, 0.6141, 0.9622,\n",
      "         0.2903],\n",
      "        [0.3061, 0.5966, 0.6947, 0.1142, 0.4289, 0.5260, 0.0081, 0.1892, 0.0706,\n",
      "         0.4165]], dtype=torch.float64), tensor([0.9684, 0.9588, 0.2900, 0.3641])]\n",
      "22 [tensor([[0.8327, 0.4988, 0.1769, 0.1821, 0.3335, 0.5610, 0.0590, 0.8834, 0.3148,\n",
      "         0.6004],\n",
      "        [0.3295, 0.0430, 0.6521, 0.7541, 0.9501, 0.3943, 0.5876, 0.2528, 0.0873,\n",
      "         0.2175],\n",
      "        [0.1276, 0.9265, 0.6975, 0.4316, 0.0389, 0.8939, 0.1228, 0.5967, 0.6575,\n",
      "         0.5744],\n",
      "        [0.2660, 0.0030, 0.0974, 0.7121, 0.5827, 0.0655, 0.6643, 0.4381, 0.4736,\n",
      "         0.8371]], dtype=torch.float64), tensor([0.2574, 0.6135, 0.1894, 0.6514])]\n",
      "23 [tensor([[0.4053, 0.1511, 0.5655, 0.1281, 0.3542, 0.9074, 0.6795, 0.4219, 0.8587,\n",
      "         0.1115],\n",
      "        [0.3616, 0.6983, 0.7862, 0.9551, 0.7232, 0.4732, 0.7181, 0.1256, 0.1425,\n",
      "         0.5421],\n",
      "        [0.4656, 0.7250, 0.4243, 0.1131, 0.1359, 0.8196, 0.2175, 0.3068, 0.5968,\n",
      "         0.2714],\n",
      "        [0.2877, 0.1604, 0.2136, 0.9833, 0.6199, 0.1718, 0.1431, 0.0228, 0.7705,\n",
      "         0.0906]], dtype=torch.float64), tensor([0.7270, 0.3178, 0.8797, 0.0240])]\n",
      "24 [tensor([[0.2240, 0.3758, 0.7814, 0.4651, 0.6492, 0.0768, 0.8373, 0.2517, 0.9139,\n",
      "         0.6306],\n",
      "        [0.7105, 0.5395, 0.8979, 0.3908, 0.1572, 0.8038, 0.3904, 0.6060, 0.7106,\n",
      "         0.7343],\n",
      "        [0.0388, 0.6404, 0.1466, 0.5375, 0.5970, 0.4190, 0.8320, 0.2127, 0.8052,\n",
      "         0.6885],\n",
      "        [0.9285, 0.7994, 0.1762, 0.4983, 0.1880, 0.8286, 0.4540, 0.0155, 0.6583,\n",
      "         0.1379]], dtype=torch.float64), tensor([0.2124, 0.2653, 0.0033, 0.6195])]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 10)\n",
    "y = np.random.rand(100)\n",
    "\n",
    "dataset = FakeDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-2623d7a959ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = transforms.Compose([transforms.ToTensor(),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-2df7e2fcd1f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                             \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                            )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/public/apps/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/public/apps/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# process and save as torch files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/public/apps/anaconda3/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting {} to {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mextract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/public/apps/anaconda3/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mextract_archive\u001b[0;34m(from_path, to_path, remove_finished)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mto_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mout_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #Âä†ËΩΩnn‰∏≠ÁöÑÂäüËÉΩÂáΩÊï∞\n",
    "import torch.optim as optim #Âä†ËΩΩ‰ºòÂåñÂô®ÊúâÂÖ≥ÂåÖ\n",
    "import torch.utils.data as Data\n",
    "from torchvision import datasets,transforms #Âä†ËΩΩËÆ°ÁÆóÊú∫ËßÜËßâÊúâÂÖ≥ÂåÖ\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5])])\n",
    "\n",
    "data_train = datasets.MNIST(root = \"./data/\",\n",
    "                            transform=transform,\n",
    "                            train = True,\n",
    "                            download = True\n",
    "                           )\n",
    "\n",
    "data_test = datasets.MNIST(root=\"./data/\",\n",
    "                           transform = transform,\n",
    "                           train = False)\n",
    "\n",
    "#Âä†ËΩΩÂ∞èÊâπÊ¨°Êï∞ÊçÆÔºåÂç≥Â∞ÜMNISTÊï∞ÊçÆÈõÜ‰∏≠ÁöÑdataÂàÜÊàêÊØèÁªÑbatch_sizeÁöÑÂ∞èÂùóÔºåshuffleÊåáÂÆöÊòØÂê¶ÈöèÊú∫ËØªÂèñ\n",
    "train_loader = Data.DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = Data.DataLoader(dataset=test_dataset,batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "#ÂÆö‰πâÁΩëÁªúÊ®°Âûã‰∫¶Âç≥Net ËøôÈáåÂÆö‰πâ‰∏Ä‰∏™ÁÆÄÂçïÁöÑÂÖ®ËøûÊé•Â±Ç784->10\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.linear1 = nn.Linear(784,10)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return F.relu(self.linear1(X))\n",
    "\n",
    "model = Model() #ÂÆû‰æãÂåñÂÖ®ËøûÊé•Â±Ç\n",
    "loss = nn.CrossEntropyLoss() #ÊçüÂ§±ÂáΩÊï∞ÈÄâÊã©Ôºå‰∫§ÂèâÁÜµÂáΩÊï∞\n",
    "optimizer = optim.SGD(model.parameters(),lr = 0.1)\n",
    "num_epochs = 5\n",
    "\n",
    "#‰ª•‰∏ãÂõõ‰∏™ÂàóË°®ÊòØ‰∏∫‰∫ÜÂèØËßÜÂåñÔºàÊöÇÊú™ÂÆûÁé∞Ôºâ\n",
    "losses = []\n",
    "acces = []\n",
    "eval_losses = []\n",
    "eval_acces = []\n",
    "\n",
    "\n",
    "\n",
    "for echo in range(num_epochs):\n",
    "    train_loss = 0   #ÂÆö‰πâËÆ≠ÁªÉÊçüÂ§±\n",
    "    train_acc = 0    #ÂÆö‰πâËÆ≠ÁªÉÂáÜÁ°ÆÂ∫¶\n",
    "    model.train()    #Â∞ÜÁΩëÁªúËΩ¨Âåñ‰∏∫ËÆ≠ÁªÉÊ®°Âºè\n",
    "    for i,(X,label) in enumerate(train_loader):     #‰ΩøÁî®Êûö‰∏æÂáΩÊï∞ÈÅçÂéÜtrain_loader\n",
    "        X = X.view(-1,784)       #X:[64,1,28,28] -> [64,784]Â∞ÜXÂêëÈáèÂ±ïÂπ≥\n",
    "        X = Variable(X)          #ÂåÖË£ÖtensorÁî®‰∫éËá™Âä®Ê±ÇÊ¢ØÂ∫¶\n",
    "        label = Variable(label)\n",
    "        out = model(X)           #Ê≠£Âêë‰º†Êí≠\n",
    "        lossvalue = loss(out,label)#Ê±ÇÊçüÂ§±ÂÄº\n",
    "        optimizer.zero_grad()       #‰ºòÂåñÂô®Ê¢ØÂ∫¶ÂΩíÈõ∂\n",
    "        lossvalue.backward()    #ÂèçÂêëËΩ¨Êí≠ÔºåÂà∑Êñ∞Ê¢ØÂ∫¶ÂÄº\n",
    "        optimizer.step()        #‰ºòÂåñÂô®ËøêË°å‰∏ÄÊ≠•ÔºåÊ≥®ÊÑèoptimizerÊêúÈõÜÁöÑÊòØmodelÁöÑÂèÇÊï∞\n",
    "\n",
    "        #ËÆ°ÁÆóÊçüÂ§±\n",
    "        train_loss += float(lossvalue)\n",
    "        #ËÆ°ÁÆóÁ≤æÁ°ÆÂ∫¶\n",
    "        _,pred = out.max(1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        acc = int(num_correct) / X.shape[0]\n",
    "        train_acc += acc\n",
    "\n",
    "    losses.append(train_loss / len(train_loader))\n",
    "    acces.append(train_acc / len(train_loader))\n",
    "    print(\"echo:\"+' ' +str(echo))\n",
    "    print(\"lose:\" + ' ' + str(train_loss / len(train_loader)))\n",
    "    print(\"accuracy:\" + ' '+str(train_acc / len(train_loader)))\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    model.eval() #Ê®°ÂûãËΩ¨Âåñ‰∏∫ËØÑ‰º∞Ê®°Âºè \n",
    "    for X,label in test_loader:\n",
    "        X = X.view(-1,784)\n",
    "        X = Variable(X)\n",
    "        label = Variable(label)\n",
    "        testout = model(X)   #argmax()\n",
    "        testloss = loss(testout,label)\n",
    "        eval_loss += float(testloss)\n",
    "\n",
    "        _,pred = testout.max(1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        acc = int(num_correct) / X.shape[0]\n",
    "        eval_acc += acc\n",
    "\n",
    "    eval_losses.append(eval_loss / len(test_loader))\n",
    "    eval_acces.append(eval_acc / len(test_loader))\n",
    "    print(\"testlose: \" + str(eval_loss/len(test_loader)))\n",
    "    print(\"testaccuracy:\" + str(eval_acc/len(test_loader)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class pytorch_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pytorch_Net, self).__init__()\n",
    "        self.num_channels = 1\n",
    "        self.image_size = 28\n",
    "        self.num_labels = 10\n",
    "        self.conv2d_1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2d_2 = nn.Conv2d(32, 32, 3, 1)\n",
    "        self.conv2d_3 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv2d_4 = nn.Conv2d(64, 64, 3, 1)\n",
    "\n",
    "        self.dense_1 = nn.Linear(4 * 4 * 64, 200)\n",
    "        self.dense_2 = nn.Linear(200, 200)\n",
    "        self.dense_3 = nn.Linear(200, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv2d_1(x))\n",
    "        x = F.relu(self.conv2d_2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2d_3(x))\n",
    "        x = F.relu(self.conv2d_4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.permute((0, 2, 3, 1))\n",
    "\n",
    "        x = x.contiguous().view(-1, 4 * 4 * 64)\n",
    "        x = x.view(-1, 4 * 4 * 64)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = F.relu(self.dense_2(x))\n",
    "        x = self.dense_3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class pytorch_keras_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pytorch_keras_Net, self).__init__()\n",
    "        self.num_channels = 1\n",
    "        self.image_size = 28\n",
    "        self.num_labels = 10\n",
    "        self.conv2d_1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2d_2 = nn.Conv2d(32, 32, 3, 1)\n",
    "        self.conv2d_3 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.conv2d_4 = nn.Conv2d(64, 64, 3, 1)\n",
    "\n",
    "        self.dense_1 = nn.Linear(4 * 4 * 64, 200)\n",
    "        self.dense_2 = nn.Linear(200, 200)\n",
    "        self.dense_3 = nn.Linear(200, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv2d_1(x))\n",
    "        x = F.relu(self.conv2d_2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2d_3(x))\n",
    "        x = F.relu(self.conv2d_4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.permute((0, 2, 3, 1))\n",
    "\n",
    "        x = x.contiguous().view(-1, 4 * 4 * 64)\n",
    "        x = x.view(-1, 4 * 4 * 64)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = F.relu(self.dense_2(x))\n",
    "        x = self.dense_3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # print(\"______data.shape:\", data.shape)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--load_keras', type=bool, default=False,\n",
    "                        help='if the model is load from keras')\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--save-model', action='store_true', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "    if args.load_keras:\n",
    "        model = pytorch_keras_Net.to(device)\n",
    "        model.load_state_dict(torch.load(\"pyt_model.pt\"))\n",
    "    else:\n",
    "        model = pytorch_Net().to(device)\n",
    "    # model.load_state_dict(torch.load(\"mnist_cnn.pt\"))\n",
    "    for name, param in model.named_parameters():\n",
    "        print(\"name:\", name)\n",
    "        print(\"param:\", param.shape)\n",
    "\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader)\n",
    "\n",
    "    if (args.save_model):\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
